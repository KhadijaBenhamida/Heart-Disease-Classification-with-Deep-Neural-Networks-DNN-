# -*- coding: utf-8 -*-
"""Heart Disease_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EA34fgwUxoZKD3xiGzzSloZKckFdi4_p
"""

!apt-get install git

!git clone https://github.com/KhadijaBenhamida/Heart-Disease-Classification-with-Deep-Neural-Networks-DNN-.git

# Commented out IPython magic to ensure Python compatibility.
# %cd Heart-Disease-Classification-with-Deep-Neural-Networks-DNN-

!git config --global user.email "khadija.benhamida2003@gmail.com"
!git config --global user.name "KhadijaBenhamida"

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c heart-disease-predictor-xm

import zipfile
import os
import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
# Unzipping the dataset
data_path = '/content/heart-disease-predictor-xm.zip'
with zipfile.ZipFile(data_path, 'r') as zip_ref:
    zip_ref.extractall('/content/heart_disease')

# Listing files in the extracted folder
extracted_files = os.listdir('/content/heart_disease')
print(extracted_files)

# Load the desired CSV file
train= pd.read_csv('/content/heart_disease/heart_disease_train.csv')
test=pd.read_csv('/content/heart_disease/heart_disease_test.csv')
print(train.head())
print(test.head())

print(train.isnull().sum())

print(train.duplicated().unique())

train.describe()

train.columns

train.head()

from sklearn.preprocessing import StandardScaler
numeric_cols=['age', 'resting.bp.s', 'cholesterol', 'max.heart.rate', 'oldpeak']
scaler=StandardScaler()
train[numeric_cols]=scaler.fit_transform(train[numeric_cols])
train.head()

x=train.drop(columns=['ID','target'])
y=train['target']

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
print(x_train.shape,x_test.shape)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize the model
model = DecisionTreeClassifier(random_state=42)

# Train the model
model.fit(x_train, y_train)

# Make predictions on the test set
y_pred = model.predict(x_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix, accuracy_score

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

"""# **RandomForestClassifier**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize the Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Train the model
rf_model.fit(x_train, y_train)

# Make predictions
rf_y_pred = rf_model.predict(x_test)

# Evaluate the model
rf_accuracy = accuracy_score(y_test, rf_y_pred)
print(f"Random Forest Accuracy: {rf_accuracy:.2f}")
print(classification_report(y_test, rf_y_pred))

"""# **LogisticRegression**"""

from sklearn.linear_model import LogisticRegression

# Initialize the Logistic Regression model
lr_model = LogisticRegression(random_state=42)

# Train the model
lr_model.fit(x_train, y_train)

# Make predictions
lr_y_pred = lr_model.predict(x_test)

# Evaluate the model
lr_accuracy = accuracy_score(y_test, lr_y_pred)
print(f"Logistic Regression Accuracy: {lr_accuracy:.2f}")
print(classification_report(y_test, lr_y_pred))

"""# **KNeighborsClassifier**"""

from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN model
knn_model = KNeighborsClassifier()

# Train the model
knn_model.fit(x_train, y_train)

# Make predictions
knn_y_pred = knn_model.predict(x_test)

# Evaluate the model
knn_accuracy = accuracy_score(y_test, knn_y_pred)
print(f"KNN Accuracy: {knn_accuracy:.2f}")
print(classification_report(y_test, knn_y_pred))

"""# **SVC**"""

from sklearn.svm import SVC

# Initialize the SVM model
svm_model = SVC(random_state=42)

# Train the model
svm_model.fit(x_train, y_train)

# Make predictions
svm_y_pred = svm_model.predict(x_test)

# Evaluate the model
svm_accuracy = accuracy_score(y_test, svm_y_pred)
print(f"SVM Accuracy: {svm_accuracy:.2f}")
print(classification_report(y_test, svm_y_pred))

"""# **DNN**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, classification_report

# Get the input shape
input_shape = x_train.shape[1]

# Build the model
dnn_model = Sequential()

# Input Layer and First Hidden Layer
dnn_model.add(Dense(64, activation='relu', input_shape=(input_shape,)))
dnn_model.add(Dropout(0.3))

# Second Hidden Layer
dnn_model.add(Dense(32, activation='relu'))
dnn_model.add(Dropout(0.3))

# Output Layer (Binary Classification)
dnn_model.add(Dense(1, activation='sigmoid'))

# Compile the model
dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary of the model
dnn_model.summary()

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = dnn_model.fit(
    x_train,
    y_train,
    validation_data=(x_test, y_test),
    epochs=100,
    batch_size=32,
    callbacks=[early_stopping],
    verbose=2
)

# Make predictions (probability) and convert to binary (0 or 1)
dnn_y_pred_prob = dnn_model.predict(x_test)
dnn_y_pred = (dnn_y_pred_prob > 0.5).astype("int32")

# Flatten the array for accuracy_score
dnn_y_pred = dnn_y_pred.flatten()

# Evaluate the model
dnn_accuracy = accuracy_score(y_test, dnn_y_pred)
print(f"DNN Accuracy: {dnn_accuracy:.2f}")
print(classification_report(y_test, dnn_y_pred))

import matplotlib.pyplot as plt

# Plotting training & validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()
plt.show()

# Plotting training & validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()
plt.show()

models = ['Decision Tree', 'Random Forest', 'Logistic Regression', 'KNN', 'SVM', 'DNN']
accuracies = [accuracy, rf_accuracy, lr_accuracy, knn_accuracy, svm_accuracy, dnn_accuracy]

# Create a summary of the accuracies
accuracy_summary = pd.DataFrame({
    'Model': models,
    'Accuracy': accuracies
})

print(accuracy_summary)